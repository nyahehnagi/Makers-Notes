# Feedback

## Week 1 Feedback

* **What did you learn from pairing with me?**
To spot obvious rows of code where bugs might lie and to google for unknown methods
* **What do you think of the code we wrote together?**
Simple and effective
* **Could you summarise the work we did together?**
We debugged the second exercise of the debugging skills workshop which related to Ciphers
* **Was I too quiet, too talkative, or did I strike a good balance of speaking and listening?**
Good balance
* **What one thing did I explain well? What one thing did I explain not so well?**
You looked at what the problem was and deduced where the issue might lie, reducing down the code we should be investigating

---------------------------------------

* **What did we work on together?**
We worked on the Boris Bike Challenges 9 - 11
* **What do you think about the code we wrote together?**
Clean, neat and clear. Leaving in alternative ways of writing code/tests helped me understand how predicate matchers work.
* **What did you learn from pairing with me today?**
Improved my knowledge on how to interpret and write spec tests.
Learnt how to reverse git commits and pushes.
Learnt about testing behaviour and state.
Predicate matchers
* **What one thing did I do well on?**
Communicated well and clarified anything that was needed.
* **What one thing could I improve on?**
The pace was a bit faster than what I'm used to.

------------------------------------

* **What did we work on together?**
Boris bike tasks 12-18
* **What do you think about the code we wrote together?**
I think it was pretty tidy by the end - you are good at finding ways to refactor and neaten/shorten the code.
* **What did you learn from pairing with me today?**
Learnt about code blocks inside expect {} and let.
* **What one thing did I do well on?**
Good at sharing and explaining knowledge that I didn't have.
* **What one thing could I improve on?**
the lines between naviagtor and driver ended up being a bit blurred at times (also my fault too). Maybe focusing more on delivering each role in a slightly stricter manner?


------------------------------------

* **What did we work on together?**
Boris Bikes challenges 13 and 14
* **What do you think about the code we wrote together?**
It worked! I understood most of it and anything I didn't you were great at explaining and allowed me to rephrase it back to you with corrections.
* **What did you learn from pairing with me today?**
How to use constants.
* **What one thing did I do well on?**
Excellent navigating and telling me what you wanted me to write down. Thank you for your patience on my much poorer navigating.
* **What one thing could I improve on?**
Explain what your thought process is before typing so we're both on the same page,

## Week 2 Feedback

* **What did we work on together?**
The weekly pairing program to practice TDD emulating an Oyster card
* **What do you think about the code we wrote together?**
Very clean, tested and refactored.
* **What did you learn from pairing with me today?**
I learned how to rescue an error message and write a feature spec
* **What one thing did I do well on?**
Logically thinking through object creations by creating simple tests. Linking and creating gemfile. Writing clear README ans well as a doc to track our progress.
* **What one thing could I improve on?**
Could test VScode live share audio next time

* **What did we work on together?**
Oystercard pairing exercise 9-11
* **What do you think about the code we wrote together?**
Works well and implementing feature spec helped us both break down the user story better.
* **What did you learn from pairing with me today?**
I gained a better understanding of when to use doubles.
* **What one thing did I do well on?**
Using TDD well by methodically working on the current failing test rather than focusing on other errors that can surface when making changes.
* **What one thing could I improve on?**
As driver, on occasion I needed to refresh my understanding. Perhaps we could have made a few more pauses to check we are still on the same train of thought.


* **What did we work on together?**
Oystercard challenge
* **What do you think about the code we wrote together?**
Pretty awesome
* **What did you learn from pairing with me today?**
How to implement include correctly into rspec and how to create a shared_context.
* **What one thing did I do well on?**
Having a good explanation for the questions I asked
* **What one thing could I improve on?**
Nothing I can think of

## Week 3 Feedback

* **What did we work on together?**
Intro to HTTPS & Sinatra
* **What do you think about the code we wrote together?**
Good - basic at this stage, but does the task required.
* **What did you learn from pairing with me today?**
The information was generally quite new to both of us do - HTTPs & Sinatra basics were good to get to grips with.
* **What one thing did I do well on?**
Making sure we both thoroughly understood the concepts before we moved on
* **What one thing could I improve on?**
We both could probably have been better at delineating the driver/navigator roles, and better at structuring our session around breaks.

* **What did we work on together?**
Setting up the infrastructure to create a web app with Ruby, Sinatra and the CapyBara feature test framework.
* **What do you think about the code we wrote together?**
We went through the CapyBara syntax by solving a testing challenge. Then we started setting up views and app files for the "Battle" game. We got stuck at an error that ultimately was only due to a wrongly named folder, but this allowed us to apply loads of debugging processes.
* **What did you learn from pairing with me today?**
Using CapyBara from the command line to test a webpage's behaviours.
Debugging techniques.
* **What one thing did I do well on?**
You did well in explaining how the various components of the web app were supposed to interact, and how our first test was supposed to be acting. I also enjoyed the first part of the session when we were just testing out the various functionalities of CapyBara.
* **What one thing could I improve on?**
Sometimes I found it difficult to follow what was happening on your screen because there were quite a few files being written, deleted and edited simultaneously. But I could follow most of the time - and whenever I felt I was getting lost, I just asked you for a clarification, and you provided it, so that's all good. What I recommend is from time to time to ask the partner something like "are you with me?" , just in case they are getting lost but don't dare interrupting your thought processes.

* **What did we work on together?**
The battle challenge - beginning to test webpages.
* **What do you think about the code we wrote together?**
The code was well structured and I think that it is readable by others.
* **What did you learn from pairing with me today?**
Session variables and how useful they are. I also learnt about how to set up a basic test for a website.
* **What one thing did I do well on?**
You were willing to go at a pace where I was able to follow, thanks for that.
* **What one thing could I improve on?**
This is really tricky, I don't really think there was anything you could have improved on.

# External Review Feedback

Scorecard
Exercise: School results
Reviewer: Pierre Roodman
No show?: No
Language: Ruby
## I use an agile process
Rating: Steady
You have done a very good job of gathering the requirements and your use of the README.md helped to clarify any assumptions that were being made about what the expected output would be for a given input.

This helped you to encode the acceptance criteria into your code by using the examples as a basis for your tests.

I would, however, suggest using table format for the example inputs and outputs as this just makes it easier to read at a glance.


## I can model anything
Rating: Steady
You modelled your solution with a single method within a class. This was a simple enough place to start and also left your program open to expansion for any future additions to the features. By starting with a single method, you left your algorithm open to adding methods later when refactoring in order to keep your main method adhering to the single-responsibility principle.

You also stuck to Ruby naming conventions, naming your methods in snake_case and including verbs in the name that make them actionable. You used PascalCase for your class name with a noun as a name.

Your algorithm was moving in the right direction and you explained to me when the session was finished how you would attempt to move onwards from where you were. The explanation that you gave showed that you had a reasonable idea of how to progress.


## I can TDD anything
Rating: Steady
You are creating tests that are testing the behaviours of the program in terms of the inputs and expected outputs. This led to a relatively good outside-in approach which started with hardcoding the return values and from there slowly growing the algorithm as you introduced test cases that were iteratively scaling up in complexity and slowly built up the algorithm in a manner that introduced small transformations to the algorithm at a time.

You were able to start doing method extraction which was a great way to remove repetition in your code. You adhered relatively closely to the RGR cycle but skipped a refactor phase early in the development process which could have removed the hardcoded returns and generalised the algorithm for those simple test cases.


## I can program fluently
Rating: Steady
You have a relatively good understanding of how to use the terminal and set up your development environment and testing framework. You also have fluency in Ruby syntax and language constructs.


You have a good idea of what you are meant to do in order to get the problem solved and were on your way to doing so in an iterative manner. I would just suggest that perhaps you practise similar exercises to this one on a website such as CodeWars, just to get used to solving similar types of problems that require string processing and also to practice your TDD.


## I can refactor anything
Rating: Steady
You are refactoring your code, but I would suggest doing so on every refactor phase in order to make sure that each iteration of the RGR cycle produces clean, generalised code where possible. It is better to refactor sooner rather than later in order to avoid complex refactoring later on that introduces bugs to your code.


## I can debug anything
Rating: Strong
You have shown an ability to read your error messages properly and examine the backtrace in order to get a good idea of what is causing bugs to occur. You also use puts statements in order to get visibility into how the code is behaving and thereby form a feedback loop that helps you to reduce the scope of where the bug is occurring.

## I write code that is easy to change
Rating: Steady
You had your test suite properly decoupled from your implementation by making sure the tests were based solely on acceptance criteria, and not reliant on the current implementation. This makes changes to the code much easier as they will not break your test suite.

You have used sensible names for your variables, methods and class and they represented well what was contained in the variables and what methods were used for, thereby increasing readability which subsequently also makes code easier to change.
You have not made regular use of Git, committing whenever tests pass on Green or refactor phases. Doing so ensures that you have a working history of your code and that any rollbacks will return the working code. This also allows you the opportunity to keep a record of changes to your code which can assist the client with keeping track of the progress made with the program.


## I have a methodological approach to problem solving
Rating: Improving
You have prioritised core cases over edge cases which is a good way to provide immediate value to a client.

For the most part, you follow the RRGR cycle quite well, but as already mentioned, you did skip refactoring code in the early refactor phases.

The following checklist may help you while you are still refining this process.

Write a failing test.
Did you run the test?
Did it fail?
Did it fail because of an assertion?
Did it fail because of the last assertion?
Make all tests pass by doing the simplest thing that could possibly work.
Consider the resulting code. Can it be improved through refactoring? If so, do it, but make sure that all tests still pass.
Ask yourself the question, ‘what is my code currently assuming’ and think of the next simple test that will break that assumption and introduce a new failing test.
Repeat

I also suggest looking at the following resources in order to get a better idea of what this process looks like.

https://www.youtube.com/watch?v=QbNhpPQkCBs

https://blog.cleancoder.com/uncle-bob/2013/05/27/TheTransformationPriorityPremise.html


## I can justify the way I work
Rating: Steady
You were vocal throughout the session, which helped me understand your thought process. However, you deviated from your process quite a few times and did not justify those deviations. These deviations include partially testing your output and not following test-driven development. I recommend that you provide sound justifications grounded in the process you’ve learnt at Makers for an effective development workflow.


## General feedback
Well done on a good first review. You have shown a good understanding of behaviour-first testing. I suggest that you try to be a bit more comprehensive with refactoring code and get into the habit of trying to identify how to remove hardcoding on the refactor phases.